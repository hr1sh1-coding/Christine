{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zshan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/zshan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/zshan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/zshan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "source": [
    "Cleaning the data, do not run it all the time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv():\n",
    "    my_filtered_csv = pd.read_csv('./betterdata.csv', usecols=['SENTIMENT', 'TEXT'])\n",
    "    return my_filtered_csv\n",
    "\n",
    "def tokenize_tweets(my_csv):\n",
    "    tweets = my_csv.TEXT.tolist()\n",
    "    sentiments = my_csv.SENTIMENT.tolist()\n",
    "    tokenizer = WordPunctTokenizer() \n",
    "    cleaned = []\n",
    "    for i in range(0, len(tweets)):\n",
    "        text = tweets[i]\n",
    "        text = re.sub('^https?://.*[rn]*','', text)\n",
    "        text = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", text)\n",
    "        text = re.sub(\"([^\\w\\s])\", \"\", text)\n",
    "        text = tokenizer.tokenize(text)\n",
    "        element = [text, sentiments[i]]\n",
    "        cleaned.append(element)\n",
    "    return cleaned\n",
    "\n",
    "def lemmatize_sentence(tweet_tokens, stop_words = ()):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_tokens = []\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('V'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "def create_lemmatized_sent(words):\n",
    "    cleaned = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    for i in range(0, len(words)):\n",
    "        sent = lemmatize_sentence(words[i][0], stop_words)\n",
    "        if len(sent) > 0:\n",
    "            element = [sent, words[i][1]]\n",
    "            cleaned.append(element)\n",
    "    return cleaned\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def write_sent(sent):\n",
    "    cleaned = []\n",
    "    for i in sent:\n",
    "        s = \"\"\n",
    "        for j in i[0]:\n",
    "            j = str(j)\n",
    "            j = j + \" \"\n",
    "            s = s + j\n",
    "        s = remove_emoji(s)\n",
    "        element = [s, i[1]]\n",
    "        cleaned.append(element)\n",
    "    df = pd.DataFrame(cleaned)\n",
    "    df.to_csv('cleaned_data.csv', index=False)\n",
    "\n",
    "my_csv = extract_csv()\n",
    "words = tokenize_tweets(my_csv)\n",
    "sent = create_lemmatized_sent(words)\n",
    "write_sent(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Converting the the columns to usable lists in sklearn\n",
    "text = df.TEXT.tolist()\n",
    "sentiment = df.SENTIMENT.tolist()\n"
   ]
  },
  {
   "source": [
    "Fitting and converting the text into vectors which will be later used for training and testing in different models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Using word2vec google"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants used:\n",
    "import word2vec\n",
    "vec_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running command: word2phrase -train cleaned_data.csv -output cleaned-phrases -min-count 5 -threshold 100 -debug 2\n",
      "Starting training using file cleaned_data.csv\n",
      "Words processed: 600K     Vocab size: 387K  \n",
      "Vocab size (unigrams + bigrams): 219917\n",
      "Words in train file: 655354\n"
     ]
    }
   ],
   "source": [
    "# my_filtered_csv = pd.read_csv('./betterdata.csv', usecols=['SENTIMENT', 'TEXT'])\n",
    "word2vec.word2phrase('cleaned_data.csv', 'cleaned-phrases', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running command: word2vec -train cleaned-phrases -output cleaned.bin -size 100 -window 5 -sample 1e-3 -hs 0 -negative 5 -threads 12 -iter 5 -min-count 5 -alpha 0.025 -debug 2 -binary 1 -cbow 1\n",
      "Starting training using file cleaned-phrases\n",
      "Vocab size: 8275\n",
      "Words in train file: 663553\n",
      "Alpha: 0.000185  Progress: 99.56%  Words/thread/sec: 298.34k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2vec('cleaned-phrases', 'cleaned.bin', size=vec_size, binary=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running command: word2vec -train cleaned_data.csv -output cleaned-clusters.txt -size 100 -window 5 -sample 1e-3 -hs 0 -negative 5 -threads 12 -iter 5 -min-count 5 -alpha 0.025 -debug 2 -binary 0 -cbow 1 -classes 100\n",
      "Starting training using file cleaned_data.csv\n",
      "Vocab size: 7971\n",
      "Words in train file: 671900\n",
      "Alpha: 0.000237  Progress: 99.53%  Words/thread/sec: 299.19k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2clusters('cleaned_data.csv', 'cleaned-clusters.txt', 100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = word2vec.load('cleaned.bin')"
   ]
  },
  {
   "source": [
    "Averaging the vectors of all the words in the sentance to get a vector for the sentence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_values = []\n",
    "cleaned_labels = [] \n",
    "for ind_1, sentences in enumerate(text):\n",
    "    cur_sentence = [0] * vec_size\n",
    "    num_words = 0\n",
    "    for word in sentences.split(' '):\n",
    "        word.strip(' ')\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        if word not in w2v_model.vocab:\n",
    "            continue\n",
    "        cur_sentence = [a + b for a, b in zip(cur_sentence, w2v_model[word])]\n",
    "        num_words += 1\n",
    "    if num_words == 0:\n",
    "        continue\n",
    "    for ind, val in enumerate(cur_sentence):\n",
    "        cur_sentence[ind] = val / num_words\n",
    "    cur_sentence.append(sentiment[ind_1])\n",
    "    cleaned_values.append(cur_sentence)\n",
    "    cleaned_labels.append(sentiment[ind_1])\n",
    "\n"
   ]
  },
  {
   "source": [
    "Normalizing the columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cleaned_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = StandardScaler().fit_transform(df.values)\n",
    "scaled_features_df = pd.DataFrame(scaled_features, index=df.index, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features =df.sample(frac=0.8,random_state=42)\n",
    "test_features = df.drop(train_features.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_features[range(0, vec_size)]\n",
    "Y_train = train_features[vec_size]\n",
    "X_val = test_features[range(0, vec_size)]\n",
    "Y_val = test_features[vec_size]"
   ]
  },
  {
   "source": [
    "Data has been cleaned and the sentences have been converted into vectors of 100 dimensions. Now we run different models on it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR_model = LogisticRegression()\n",
    "LR_model.fit(X_train, Y_train)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ]
  },
  {
   "source": [
    "Creating a function which will take input sentence and then output the scale of depression from 0 to 4"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_clean(sentence):\n",
    "    tokenizer = WordPunctTokenizer() \n",
    "    cleaned = []\n",
    "    sentence = re.sub('^https?://.*[rn]*','', sentence)\n",
    "    sentence = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", sentence)\n",
    "    sentence = re.sub(\"([^\\w\\s])\", \"\", sentence)\n",
    "    sentence = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    cleaned = []\n",
    "    # stop_words = stopwords.words('english')\n",
    "    sent = lemmatize_sentence(sentence)\n",
    "    result = ''\n",
    "    if len(sent) > 0:\n",
    "        result = ' '.join(sent)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def depression_scale(sentence):\n",
    "    # cleaning the sentence\n",
    "    clean_sentence = sentence_clean(sentence)\n",
    "    word_vector = [0] * vec_size\n",
    "    num_words = 0\n",
    "    for word in clean_sentence.split(' '):\n",
    "        word.strip(' ')\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        if word in w2v_model.vocab:\n",
    "            word_vector = [a + b for a, b in zip(word_vector, w2v_model[word])]\n",
    "        else:\n",
    "            continue\n",
    "        num_words += 1\n",
    "    if num_words == 0:\n",
    "        return 2\n",
    "    \n",
    "    for ind, val in enumerate(word_vector):\n",
    "        word_vector[ind] = val / num_words\n",
    "    y_result_probs = 4 * LR_model.predict_proba([word_vector])[0][1]\n",
    "    return y_result_probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.5076293401651547\n"
     ]
    }
   ],
   "source": [
    "print(depression_scale(\"I am so happy that I want to die\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbasecondaaf70d140d83841baa1e502b1df292170"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}